{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae1fe1bZe-AO"
      },
      "source": [
        "# ðŸ“Š **Introduction to Machine Learning with Scikit-Learn**\n",
        "\n",
        "Welcome to this **introductory guide to Machine Learning (ML)**! This notebook covers essential ML concepts using **Scikit-Learn**, starting from data preparation to model evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFg0RjcMevhb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iotuyH-5ewZ7"
      },
      "source": [
        "## ðŸ”µ **1: What is Machine Learning?**\n",
        "\n",
        "Machine Learning (ML) is a field of computer science that focuses on finding patterns in data. In hyper-simplified non-functional code, the procedure is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnF3q2s1ewgt"
      },
      "outputs": [],
      "source": [
        "# patterns = ml_algorithm(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O25YbIjs6cpF"
      },
      "source": [
        "There are two popular kinds of problems, also called learning scenarios, that ML practiioners confront.\n",
        "\n",
        "The simplest learning scenario is called **Unsupervised Learning**. In this case we want to find some hidden structure in the data provided. An archetypal problem in unsupervised learning is **clustering**. Clustering involves taking in a collection $X$ of $n$ vectors/arrays of real numbers $\\{x_1, \\dots, x_n\\}$ where $x_i\\in R^m$ and partitioning them into disjoint groups.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H9_RSQDewmg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAslmzWa93rY"
      },
      "outputs": [],
      "source": [
        "#clusters = clustering_algorithm(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpbe6UMN7-dI"
      },
      "source": [
        "\n",
        "The other learning scenario we consider is **supervised learning**. In this case each of the *observation* $x_i$ has a matched **label** $y_i$. The assumption is that there is some function $f$ that connects the observatons to the labels, that is\n",
        "$y_i = f(x_i)$.\n",
        "\n",
        "The goal of ML in supervised learning is to *learn a model*  $m_f$ that ``behaves like $f$.\" That is we want $m_f(x_i)$ to be close to $y_i$ for all $i$. Once $m_f$ has been constructed, we can use it to predict the labels on new data $X'$ where we may not know the labels. Again in cartoon code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ieQriFN-A4V"
      },
      "outputs": [],
      "source": [
        "#model = fit_model(X,y)\n",
        "#predicted_labels = model.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp3sRncA_FsC"
      },
      "source": [
        "Supervised learning can be further subdivied into **regression problems** and **classification problems** based on the nature of their labels.\n",
        "\n",
        "Regression problems have continuous/real-valued labels. An example regression problem is predicting the price of a house from various measurements about it (e.g. size, neighborhood, number of bedrooms, etc.).\n",
        "\n",
        "For classification problems the labels from from a finite set of categories. An example classification problem is categorizing an e-mail as spam or not spam.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8kurb-a_-r3"
      },
      "source": [
        "Fortunately we do need to rewrite ML algorithms from scratch. In python there is a comprehensive library called `scikit-learn` that has implemented many popular and effective machine learning algorithms that can be used \"off the shelf.\"  We will use `scikit-learn` to illustrate the basics of machine learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0BspNwTexny"
      },
      "source": [
        "## ðŸ”µ **2: Loading & Exploring a Dataset**\n",
        "\n",
        "We are still missing a key ingredient: a dataset. For this we can use one of the many datasets included with `scikit-learn`. In particular we will use the commonly analyzed **Iris** dataset. This dataset contains information about three species of iris flowers. We will, over this and the following sections, build a classifier that inputs information and can classifies them as one of the three species of iris: *setosa*, *versicolor*, and *virginica*.\n",
        "\n",
        "\n",
        "\n",
        "In addition to `scikit-learn`, which focuses primarily on the machine learning side of things, we need two addition libraries: **NumPy** and **Pandas**. These libraries are used to manage and process data before and after analyzing it with `scikit-learn`. These libraries are installed in the normal way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gcVOs_KJexKb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bp8v8uOexBJ"
      },
      "source": [
        "Now we load the dataset proper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RwMM16hRexwF"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sIBH6QBIcxv"
      },
      "source": [
        "As written the `iris` object is a dictionary with our data and the labels (also called targets) as well as some additional information or *metadata*. As a first step let's pull out the observations $X$ and the labels $y$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3qdLC61bJFa9"
      },
      "outputs": [],
      "source": [
        "X, y = iris['data'],iris['target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P4LC3EMJfNt"
      },
      "source": [
        "Okay, now what do these look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEmf9zWAJjB8",
        "outputId": "5b8c020c-03d9-4c8e-f419-0224542f7df6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_qBL44fJm0F",
        "outputId": "0e2b6c98-e8c1-4a12-a380-daf096c1aa76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HgEwNHiJrvp"
      },
      "source": [
        "This is... not particularly useful. Let's try finding out how large the datasets are. To do this we wil use the `.shape` attribute of `NumPy` arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4m6_SwuJ3Bt",
        "outputId": "b1ddedc6-17f6-4955-9237-8655be366a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4)\n",
            "(150,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiIO1cXDKLIq"
      },
      "source": [
        "So there are 150 observations in our data and $X$ has 4 *features* per observation. It would be good to know what measure each feature (column) of $X$ maps to. We can do this with a simple call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5vr4TR2KWKZ",
        "outputId": "dbd47432-5004-4fba-f6a2-6686eb3a8e77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "iris['feature_names']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZVaXkGZKud7"
      },
      "source": [
        "It's clear from the code above that $y$ has three categories, encoded as numbers 0, 1, and 2. Finding out which species of iris these correspond to is doable with another line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve-3Vk02LGFw",
        "outputId": "9d27cf44-caa7-4c94-e3eb-8db0d819ca1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "iris['target_names']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYELk7lKhjpc"
      },
      "source": [
        "It's worth  pulling together a count of how many observations we have for each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "RzWDTDKRLd86",
        "outputId": "401edcbb-24e2-40dc-c8df-80897a058021"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "setosa        50\n",
              "versicolor    50\n",
              "virginica     50\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>setosa</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>versicolor</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>virginica</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "label_to_species = {i: iris[\"target_names\"][i] for i in range(len(iris[\"target_names\"]))}\n",
        "renamed_targets = [label_to_species[j] for j in y]\n",
        "pd.Series(renamed_targets).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aliaRuOciVbn"
      },
      "source": [
        "As a last step we want to look at features. To do this we make a pandas dataframe (which you can think of as a spreadsheet with some extra bells and whistles) and analze their properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDn5qYPQi5rf",
        "outputId": "fcd9fbca-3625-4345-e3d0-936e0332e618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Value of Each Feature\n",
            "sepal length (cm)    5.843333\n",
            "sepal width (cm)     3.057333\n",
            "petal length (cm)    3.758000\n",
            "petal width (cm)     1.199333\n",
            "dtype: float64\n",
            "\n",
            "Standard Deviation of Each Feature\n",
            "sepal length (cm)    0.828066\n",
            "sepal width (cm)     0.435866\n",
            "petal length (cm)    1.765298\n",
            "petal width (cm)     0.762238\n",
            "dtype: float64\n",
            "\n",
            "Correlation Between Features\n",
            "                   sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
            "sepal length (cm)           1.000000         -0.117570           0.871754   \n",
            "sepal width (cm)           -0.117570          1.000000          -0.428440   \n",
            "petal length (cm)           0.871754         -0.428440           1.000000   \n",
            "petal width (cm)            0.817941         -0.366126           0.962865   \n",
            "\n",
            "                   petal width (cm)  \n",
            "sepal length (cm)          0.817941  \n",
            "sepal width (cm)          -0.366126  \n",
            "petal length (cm)          0.962865  \n",
            "petal width (cm)           1.000000  \n"
          ]
        }
      ],
      "source": [
        "data = pd.DataFrame(data=X, columns=iris['feature_names'])\n",
        "print(\"Mean Value of Each Feature\")\n",
        "print(data.mean(axis=0))\n",
        "print(\"\\nStandard Deviation of Each Feature\")\n",
        "print(data.std(axis=0))\n",
        "print(\"\\nCorrelation Between Features\")\n",
        "print(data.corr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U94Ci3f4ex2e"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NugZpgBSeyp2"
      },
      "source": [
        "## ðŸ”µ **3: Splitting Data into Train & Test Sets**\n",
        "As mentioned above supervised learning is designed to predict the response (species) on *new* data. However, we only have the one dataset! In order to evaluate an algorithm on this dataset we randomly divide it into two subsets: a **training** set and a **testing** set. This partition of $X$ is called a *train-test-split*. Usually the training set is much larger than the test set. We set it to be 80% of the data in this example.\n",
        "\n",
        "`scikit-learn` has built in functionality to perform train-test splits. In order to keep things reproducible we set a random seed so that the train-test split is consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i5jBZ6vey7j",
        "outputId": "73ed5916-c46a-4453-dc9d-b5d6c4ae5340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training data has 120 observations\n",
            "The testing data has 30 observations\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#set the seed\n",
        "seed = 123456\n",
        "train_percentage = 0.8\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = train_percentage, random_state=seed)\n",
        "print(f\"The training data has {X_train.shape[0]} observations\")\n",
        "print(f\"The testing data has {X_test.shape[0]} observations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pCfQzMSBw_z"
      },
      "source": [
        "We should probably also check how many of each species are in the training and testing sets..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_names = [label_to_species[j] for j in y_train]\n",
        "print(type(training_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLlhA9J-7JAw",
        "outputId": "2c6cc225-e215-4c10-919e-ea7053b7c350"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSFL60ZACPPk",
        "outputId": "721837c0-c238-4de8-f740-1c35f1324ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The counts of species in the training dataset is:\n",
            "-----------\n",
            "setosa        42\n",
            "versicolor    41\n",
            "virginica     37\n",
            "Name: count, dtype: int64\n",
            "\n",
            "The counts of species in the testing dataset is:\n",
            "-----------\n",
            "virginica     13\n",
            "versicolor     9\n",
            "setosa         8\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "training_names = [label_to_species[j] for j in y_train]\n",
        "testing_names = [label_to_species[j] for j in y_test]\n",
        "\n",
        "print(f\"The counts of species in the training dataset is:\\n-----------\\n{pd.Series(training_names).value_counts()}\\n\")\n",
        "print(f\"The counts of species in the testing dataset is:\\n-----------\\n{pd.Series(testing_names).value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "580wzukUC_F-"
      },
      "source": [
        "We started with an an exactly equal number of each species in the dataset but we have changed this (slightly) by splitting it. This is something to note as we train our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqBh8NP1ez6G"
      },
      "source": [
        "## ðŸ”µ **4: Training a Machine Learning Model**\n",
        "Now that we've split our data it's time to train our model. We'll use a simple linear model called **logistic regression** (see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for details).\n",
        "\n",
        "\n",
        "As the names suggest we will use our training data to fit (train) the model and then pass the testing data through it for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoM__Co6e0AW",
        "outputId": "59e533ab-cba7-4009-eeb5-6c24d509cbc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 0, 1, 0, 0, 2, 2, 2, 0, 1, 2, 2, 0, 0, 2, 1, 2, 1, 0, 1, 2,\n",
              "       1, 1, 1, 2, 2, 2, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train,y_train)\n",
        "predictions = classifier.predict(X_test)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7ZAaBOgFTgE"
      },
      "source": [
        ":It is good (and common) practice to *standardize* the data by make it so each feature has mean zero and unit variance. We can use the `StandardScalar` functionality `scikit-learn` has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "krc50QxPGLhN"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "standardizer = StandardScaler()\n",
        "# note that the scaler does NOT care about the targets.\n",
        "# standardizer.fit(X_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziF_ntcAHChL"
      },
      "source": [
        "Does standardizing help? Do we do better if we preserve the class distribution when we split the train and test data? In order to answer these questions we need a way to evaluate model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8R-ZqcSe0ZI"
      },
      "source": [
        "## ðŸ”µ **5: Evaluating Model Performance**\n",
        "- Using **accuracy, precision, recall, and F1-score** for classification.\n",
        "- Using **Mean Squared Error (MSE) and RÂ² score** for regression.\n",
        "- Displaying results using `classification_report()` and `confusion_matrix()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8OyuoSSe0gH",
        "outputId": "f5afd1f4-53fe-43b3-f6a6-de92e3563426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is 100.0%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "# print(classification_report(y_test, predictions))\n",
        "print(f\"Accuracy is {100*accuracy_score(y_test,predictions)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_-qkugxIBrS"
      },
      "source": [
        "How nice! We got 100% accuracy. This, however, doesn't tell us very much. Let's try this again but with a smaller percentage of the data used for training. We check across a range of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcDbsfakJd4H",
        "outputId": "f0c5b9c6-7637-4f7f-8bf9-dbbbcb3c1a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Percentage: 30.0 % ---- Accuracy:96.19%\n",
            "Train Percentage: 35.0 % ---- Accuracy:97.96%\n",
            "Train Percentage: 40.0 % ---- Accuracy:97.78%\n",
            "Train Percentage: 45.0 % ---- Accuracy:97.59%\n",
            "Train Percentage: 50.0 % ---- Accuracy:97.33%\n",
            "Train Percentage: 55.0 % ---- Accuracy:97.06%\n",
            "Train Percentage: 60.0 % ---- Accuracy:98.33%\n",
            "Train Percentage: 65.0 % ---- Accuracy:98.11%\n",
            "Train Percentage: 70.0 % ---- Accuracy:97.78%\n",
            "Train Percentage: 75.0 % ---- Accuracy:100.0%\n",
            "Train Percentage: 80.0 % ---- Accuracy:100.0%\n",
            "Train Percentage: 85.0 % ---- Accuracy:100.0%\n",
            "Train Percentage: 90.0 % ---- Accuracy:100.0%\n",
            "Train Percentage: 95.0 % ---- Accuracy:100.0%\n"
          ]
        }
      ],
      "source": [
        "train_percentages = np.round((10**-1)*np.arange(3,10,0.5),2)\n",
        "# increase the total number of iterations used to fit the model\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "for tpct in train_percentages:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = tpct, random_state=seed)\n",
        "  classifier.fit(X_train,y_train)\n",
        "  predictions = classifier.predict(X_test)\n",
        "  print(f\"Train Percentage: {np.round(100*tpct,2)} % ---- Accuracy:{np.round(100*accuracy_score(y_test,predictions),2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxMaXiaGL2Qz"
      },
      "source": [
        "So, as one would expect as we increase the amount of data used for model training the better the model does. However, even at low training data we do quite well. Let's try this again but this time with the class distributions preserved. We will use this with the `stratify` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQRMfDRFMjRo",
        "outputId": "d153ed5e-26e5-410a-e4b6-54026d16ba99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For stratified splits...\n",
            "Train Percentage: 30.0 % ---- Accuracy:94.29%\n",
            "Train Percentage: 35.0 % ---- Accuracy:93.88%\n",
            "Train Percentage: 40.0 % ---- Accuracy:94.44%\n",
            "Train Percentage: 45.0 % ---- Accuracy:95.18%\n",
            "Train Percentage: 50.0 % ---- Accuracy:93.33%\n",
            "Train Percentage: 55.0 % ---- Accuracy:92.65%\n",
            "Train Percentage: 60.0 % ---- Accuracy:91.67%\n",
            "Train Percentage: 65.0 % ---- Accuracy:90.57%\n",
            "Train Percentage: 70.0 % ---- Accuracy:93.33%\n",
            "Train Percentage: 75.0 % ---- Accuracy:92.11%\n",
            "Train Percentage: 80.0 % ---- Accuracy:90.0%\n",
            "Train Percentage: 85.0 % ---- Accuracy:91.3%\n",
            "Train Percentage: 90.0 % ---- Accuracy:86.67%\n",
            "Train Percentage: 95.0 % ---- Accuracy:100.0%\n"
          ]
        }
      ],
      "source": [
        "print(\"For stratified splits...\")\n",
        "for tpct in train_percentages:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = tpct, random_state=seed,stratify=y)\n",
        "  classifier.fit(X_train,y_train)\n",
        "  predictions = classifier.predict(X_test)\n",
        "  print(f\"Train Percentage: {np.round(100*tpct,2)} % ---- Accuracy:{np.round(100*accuracy_score(y_test,predictions),2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9C28llIMwLA"
      },
      "source": [
        "Interesting! If we force the class distribution to be preserved then the model seems to struggle (more). We also see that the relationship between amount of training data and performance is less consistent. What if we try standardizing our data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjrGSfOpOYwJ",
        "outputId": "9fe491b2-98b4-4b30-b34b-9bfb77f13e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For stratified and standardized splits...\n",
            "Train Percentage: 30.0 % ---- Accuracy:94.29%\n",
            "Train Percentage: 35.0 % ---- Accuracy:94.9%\n",
            "Train Percentage: 40.0 % ---- Accuracy:95.56%\n",
            "Train Percentage: 45.0 % ---- Accuracy:95.18%\n",
            "Train Percentage: 50.0 % ---- Accuracy:94.67%\n",
            "Train Percentage: 55.0 % ---- Accuracy:94.12%\n",
            "Train Percentage: 60.0 % ---- Accuracy:93.33%\n",
            "Train Percentage: 65.0 % ---- Accuracy:92.45%\n",
            "Train Percentage: 70.0 % ---- Accuracy:93.33%\n",
            "Train Percentage: 75.0 % ---- Accuracy:92.11%\n",
            "Train Percentage: 80.0 % ---- Accuracy:90.0%\n",
            "Train Percentage: 85.0 % ---- Accuracy:91.3%\n",
            "Train Percentage: 90.0 % ---- Accuracy:86.67%\n",
            "Train Percentage: 95.0 % ---- Accuracy:100.0%\n"
          ]
        }
      ],
      "source": [
        "print(\"For stratified and standardized splits...\")\n",
        "for tpct in train_percentages:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = tpct, random_state=seed,stratify=y)\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(X_train)\n",
        "  X_train = scaler.transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "  classifier.fit(X_train,y_train)\n",
        "  predictions = classifier.predict(X_test)\n",
        "  print(f\"Train Percentage: {np.round(100*tpct,2)} % ---- Accuracy:{np.round(100*accuracy_score(y_test,predictions),2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8plHkBiOwEI"
      },
      "source": [
        "So standardizing doesn't do much. However, this is likely due to the fact that the features are all measured in the same units (cm) and are roughly of the same scale. However, this is not usually true of the datasets we see in practice. In general:\n",
        "\n",
        "- Your **default** move should be to standardize your data.\n",
        "- Standardization **must** be done on the train set. If you fit `StandardScaler` on the full data then information will leak and you will over-estimate model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMdcCc7Pe08N"
      },
      "source": [
        "## ðŸ”µ **6: Hyperparameter Tuning**\n",
        "If you look over the logistic regression documentation you'll see two parameters; a \"penalty\" parameter and a parameter \"C\". We are going to try to choose these values (called **hyperparameters**) in order to maximize model performance. In order to do this we will avail ourselves of two useful features that `scikit-learn` has: `Pipeline` and `GridSearchCV`. We will use `Pipeline` to wrap our train-test split protocol and `GridSearchCV` to try all possible combinations of hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LEgbAmwdhOwU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# The Pipeline is a named list of steps that get applied in order.\n",
        "# Since we will be trying multiple penalities we need to specify a\n",
        "# solver that can handle both.\n",
        "model_pipe = Pipeline([('scale',StandardScaler()),('clf',LogisticRegression(solver='liblinear',max_iter=1000))])\n",
        "\n",
        "# Now we specify the parameters to search over.\n",
        "# This takes a form of a dictionary where the parameter names are\n",
        "# taken from the document and we add a prefix so that\n",
        "# GridSearchCV knows to which step the parameters belong.\n",
        "# These need to come after two underscores.\n",
        "\n",
        "param_grid = {\n",
        "    'clf__C':np.logspace(-4, 4, 100),\n",
        "    'clf__penalty':['l1','l2']}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TmxWRRQjFVU"
      },
      "source": [
        "In order to choose hyperperparameters we first split the data then use what is called stratified k-fold cross validation. In this process the training data is split into k equal subsets, the model is then fit to the remaining k-1 subsets (folds) and fit to the hold out. The model parameters witht the best average performance over all k folds are kept as the best. We choose $k=5$ because our dataset is small. `scikit-learn` uses stratified kfold splits by default for classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv4JT-bKjbTM",
        "outputId": "0f51058f-6da8-49e9-95f3-cba7b4f06277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For stratified and standardized splits...\n",
            "Train Percentage: 30.0 % ---- Accuracy:94.29%\n",
            "Train Percentage: 35.0 % ---- Accuracy:95.92%\n",
            "Train Percentage: 40.0 % ---- Accuracy:95.56%\n",
            "Train Percentage: 45.0 % ---- Accuracy:95.18%\n",
            "Train Percentage: 50.0 % ---- Accuracy:93.33%\n",
            "Train Percentage: 55.0 % ---- Accuracy:94.12%\n",
            "Train Percentage: 60.0 % ---- Accuracy:93.33%\n",
            "Train Percentage: 65.0 % ---- Accuracy:92.45%\n",
            "Train Percentage: 70.0 % ---- Accuracy:93.33%\n",
            "Train Percentage: 75.0 % ---- Accuracy:92.11%\n",
            "Train Percentage: 80.0 % ---- Accuracy:90.0%\n",
            "Train Percentage: 85.0 % ---- Accuracy:91.3%\n",
            "Train Percentage: 90.0 % ---- Accuracy:93.33%\n",
            "Train Percentage: 95.0 % ---- Accuracy:100.0%\n"
          ]
        }
      ],
      "source": [
        "model = GridSearchCV(model_pipe,param_grid,cv=5)\n",
        "print(\"For stratified and standardized splits...\")\n",
        "for tpct in train_percentages:\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = tpct, random_state=seed,stratify=y)\n",
        "  model.fit(X_train,y_train)\n",
        "\n",
        "  predictions = model.predict(X_test)\n",
        "  print(f\"Train Percentage: {np.round(100*tpct,2)} % ---- Accuracy:{np.round(100*accuracy_score(y_test,predictions),2)}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-sixTeOoKeU"
      },
      "source": [
        "We see that tuning the hyperparameters increases our performance, albeit modestly. Once we have fit the model we can use the best hyper-parameters saved to re-fit on the same model. Let's re-do this with 60% of the data held out. The `GridSearchCV` object has an attribute called `best_estimator_` that stores the model with the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UvvVyYKo0MG",
        "outputId": "46b0eecb-aad7-4fac-8a59-77b3d96a1d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline(steps=[('scale', StandardScaler()),\n",
            "                ('clf',\n",
            "                 LogisticRegression(C=np.float64(7.054802310718645),\n",
            "                                    max_iter=1000, penalty='l1',\n",
            "                                    solver='liblinear'))])\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 0.6, random_state=seed,stratify=y)\n",
        "model.fit(X_train,y_train)\n",
        "print(model.best_estimator_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7cigKeue2Cl"
      },
      "source": [
        "## ðŸ”µ **7: Saving & Loading a Trained Model**\n",
        "Once we've built and trained our model we want to save it for future use so that we (or others) won't need to re-train it. There are many [options](https://scikit-learn.org/stable/model_persistence.html) for this in python. We'll use `joblib` for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu0H2UkNe2JV",
        "outputId": "2000a6fb-0e06-417f-b306-8fbf6b5a5a5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 1, 2, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 2, 2, 2, 0, 2, 1, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 2, 2, 2, 1, 1, 1, 2, 2, 0, 0, 1, 0, 0, 1, 0, 1,\n",
              "       1, 1, 0, 1, 2, 2, 2, 1, 0, 0, 2, 1, 1, 1, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "best_model = model.best_estimator_\n",
        "joblib.dump(best_model,\"./my_model.joblib\")\n",
        "loaded_model = joblib.load(\"./my_model.joblib\")\n",
        "loaded_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oVwO-RUp1JU"
      },
      "source": [
        "Alright! The model works and can be loaded. The last step, if we want to apply to genuinely new data, is to re-fit the best model on all our data and save it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmVAjFGhp8ht",
        "outputId": "f573ad3d-0c10-4776-817d-09b29907c8cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./production_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "production_model = model.best_estimator_\n",
        "production_model.fit(X,y)\n",
        "joblib.dump(production_model,\"./production_model.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vjl6cC1e2PP"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}